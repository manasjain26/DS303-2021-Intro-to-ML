{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1 align=center> DS303: Naive Bayes Classifier </h1>\n",
    "\n",
    "A demonstrative jupyter notebook on Naive Bayes Classifier. It has been divided into two parts-\n",
    "\n",
    "- Gaussian Naive Bayes (using [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris))\n",
    "- Multinomial Naive Bayes (using [Spambase Dataset](http://archive.ics.uci.edu/ml/datasets/Spambase/))\n",
    "\n",
    "As we know the Naive Bayes classifier works as follows- \n",
    "\n",
    "$$\n",
    "\\hat{y} = arg\\max_{y} P(y)\\prod_{i = 1}^{N}P(x_i|y)\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the prediction and P(y) is the prior class probability which is fixed for a given data. But, different approaches can be used for calculating likelihoods. Two of them have been implemented below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "<h1 align=center>Gaussian Naive Bayes</h1>\n",
    "\n",
    "In this variation of Naive Bayes, the likelihoods are calculated using Gaussian distribution function as-\n",
    "\n",
    "$$\n",
    "P(x_i|y) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_y}e^{\\frac{-(x_i - y)^2}{2\\sigma_y^2}}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Model\n",
    "class MyGaussianNB:\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        np.random.seed(1234)\n",
    "        self.data = pd.read_csv(filepath, header=None, names=['sl', 'sw', 'pl', 'pw', 'labels'])    # read data\n",
    "        self.class_probabs = dict()\n",
    "        self.n_classes = dict()\n",
    "        self.summary_by_class = dict()\n",
    "\n",
    "    def f_X(self, x, mean=0, std=1):    # Gaussian pdf\n",
    "        return  (1 / (np.math.sqrt(2 * np.math.pi) * std)) * np.math.exp(-((x - mean)**2) / (2 * std**2))\n",
    "\n",
    "    def preprocess(self, split=0.8):\n",
    "        indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.train_data, self.test_data = self.data.iloc[indices[:int(len(indices) * split)], :], self.data.iloc[indices[int(len(indices) * split):], :]    # split data into train and test sets\n",
    "        grouped = self.train_data.groupby('labels') # group data by class labels\n",
    "\n",
    "        # compute the classwise mean and std for all features\n",
    "        self.summary_by_class['mean'] = grouped.mean()  \n",
    "        self.summary_by_class['std'] = grouped.std()\n",
    "\n",
    "        # split the features and labels\n",
    "        self.X_train, self.y_train = self.train_data.iloc[:, :-1], self.train_data.iloc[:, -1]\n",
    "        self.X_test, self.y_test = self.test_data.iloc[:, :-1], self.test_data.iloc[:, -1]\n",
    "\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "    def fit(self):\n",
    "        # compute class prior probabilities\n",
    "        self.class_probabs = self.y_train.value_counts(normalize=True)\n",
    "        print(f'Class Probabilities:\\n{self.class_probabs}')\n",
    "\n",
    "    def predict(self, x):\n",
    "        keys = ['sl', 'sw', 'pl', 'pw']\n",
    "        p_vers = self.class_probabs['Iris-versicolor']\n",
    "        p_set = self.class_probabs['Iris-setosa']\n",
    "        p_vir = self.class_probabs['Iris-virginica']\n",
    "        for i, key in enumerate(keys):\n",
    "            p_vers *= self.f_X(x[i], self.summary_by_class['mean'][key]['Iris-versicolor'], self.summary_by_class['std'][key]['Iris-versicolor'])   # compute the probability for Iris-versicolor class\n",
    "            p_set *= self.f_X(x[i], self.summary_by_class['mean'][key]['Iris-setosa'], self.summary_by_class['std'][key]['Iris-setosa'])    # compute the probability for Iris-setosa class\n",
    "            p_vir *= self.f_X(x[i], self.summary_by_class['mean'][key]['Iris-virginica'], self.summary_by_class['std'][key]['Iris-virginica'])  # compute the probability for Iris-virginica class\n",
    "\n",
    "        # predict\n",
    "        if p_vers > p_set and p_vers > p_vir:\n",
    "            return 'Iris-versicolor'\n",
    "        elif p_vir > p_set and p_vir > p_vers:\n",
    "            return 'Iris-virginica'\n",
    "        elif p_set > p_vers and p_set > p_vir:\n",
    "            return 'Iris-setosa'\n",
    "        else:\n",
    "            return 'Can\\'t determine'\n",
    "        \n",
    "    def test(self):\n",
    "        # test the model\n",
    "        predictions = []\n",
    "        for i in range(self.X_test.shape[0]):\n",
    "            predictions.append(self.predict(self.X_test.iloc[i, :]))\n",
    "        predictions = np.array(predictions)\n",
    "        return (predictions == self.y_test).sum() / len(predictions)"
   ]
  },
  {
   "source": [
    "## Processing data and fitting model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class Probabilities:\nIris-versicolor    0.433333\nIris-setosa        0.300000\nIris-virginica     0.266667\nName: labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gnb = MyGaussianNB('iris.data')\n",
    "X_train, y_train, X_test, y_test = gnb.preprocess(split=0.2)\n",
    "gnb.fit()"
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {gnb.test()}')"
   ]
  },
  {
   "source": [
    "## sklearn Gaussian Naive Bayes model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "# use sklearn model to predict\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_iri, y_train_iris)\n",
    "print(f'Accuracy = {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "source": [
    "<h1 align=center>Spam Filtering using Multinomial Naive Bayes</h1>\n",
    "\n",
    "For spam filtering, we use another variation of Naive Bayes i.e. Multinomial Naive Bayes Clssifier where the likelihoods are computed as-\n",
    "\n",
    "$$\n",
    "P(w_i|\\lambda) = \\dfrac{N_{w_i|\\lambda} + \\alpha}{N_{\\lambda} + \\alpha N_{vocabulary}}\n",
    "$$\n",
    "\n",
    "where $w_i$ is the word whose likelihood is to be calculated and $\\lambda$ represents a class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Model\n",
    "class MyMultinomialNB:\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        np.random.seed(1234)\n",
    "        self.data = pd.read_csv(filepath, sep='\\t', header=None, names=['labels', 'sms'])   # read data\n",
    "        print(self.data.shape)\n",
    "        print(self.data.head(), end='\\n\\n')\n",
    "        self.class_probabs = dict()\n",
    "        self.vocab = None\n",
    "        self.word_count = None\n",
    "        self.num_spam = 0\n",
    "        self.num_ham = 0\n",
    "        self.vocab_size = 0\n",
    "        self.alpha = 1\n",
    "\n",
    "    def preprocess(self, split=0.8):\n",
    "        self.data['sms'] = self.data['sms'].str.replace('\\W+', ' ')   # replace non-word characters with a space\n",
    "        self.data['sms'] = self.data['sms'].str.replace('\\s+', ' ')   # relace multiple spaces with single space\n",
    "        self.data['sms'] = self.data['sms'].str.strip()   # remove leading and trailing whitespaces\n",
    "        self.data['sms'] = self.data['sms'].str.lower()   # make all characters lower case\n",
    "        self.data['sms'] = self.data['sms'].str.split()   # break words on spaces to form a list\n",
    "        # print(self.data.head())\n",
    "\n",
    "        indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.train_data, self.test_data = self.data.iloc[indices[:int(split * len(indices))], :].reset_index(drop=True), self.data.iloc[indices[int(split * len(indices)):], :].reset_index(drop=True)  # split train and test data\n",
    "        \n",
    "        print(f'Training data: {self.train_data.shape[0]}')\n",
    "        print(f'Test data: {self.test_data.shape[0]}')\n",
    "\n",
    "        # split message and labels\n",
    "        self.X_train, self.y_train = self.train_data.iloc[:, 1], self.train_data.iloc[:, 0] \n",
    "        self.X_test, self.y_test = self.test_data.iloc[:, 1], self.test_data.iloc[:, 0]\n",
    "\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "    \n",
    "    def fit(self):\n",
    "        self.class_probabs = self.y_train.value_counts(normalize=True)  # calculate class prior probabilities\n",
    "        print(f'Class Probabilities:\\n{self.class_probabs}')\n",
    "        self.vocab = list(set(self.X_train.sum()))  # create vocabulary\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f'Vocabulary length: {self.vocab_size}')\n",
    "        self.word_count = pd.DataFrame([[self.X_train.iloc[i].count(word) for word in self.vocab] for i in range(self.X_train.shape[0])], columns=self.vocab)   # count the number of times each word has appeare in a message\n",
    "        self.X_train = pd.concat([self.X_train, self.word_count], axis=1).iloc[:, 1:]   # update train features\n",
    "        # print(self.X_train.head())\n",
    "    \n",
    "    # calculate likelihood of a word for spam class\n",
    "    def p_w_spam(self, word):\n",
    "        if word in self.vocab:\n",
    "            return (self.X_train.loc[self.y_train == 'spam', word].sum() + self.alpha) / ((self.y_train == 'spam').sum() + self.alpha * self.vocab_size)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # calculate likelihood of a word for ham class\n",
    "    def p_w_ham(self, word):\n",
    "        if word in self.vocab:\n",
    "            return (self.X_train.loc[self.y_train == 'ham', word].sum() + self.alpha) / ((self.y_train == 'ham').sum() + self.alpha * self.vocab_size)\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def predict(self, text):\n",
    "        p_spam = self.class_probabs['spam']\n",
    "        p_ham = self.class_probabs['ham']\n",
    "        for word in text:\n",
    "            p_spam *= self.p_w_spam(word)   # compute probability of message belonging to spam\n",
    "            p_ham *= self.p_w_ham(word) # compute probability of message belonging to ham\n",
    "\n",
    "        # predict\n",
    "        if p_spam > p_ham:\n",
    "            return 'spam'\n",
    "        elif p_ham > p_spam:\n",
    "            return 'ham'\n",
    "        else:\n",
    "            return 'can not classify'\n",
    "        \n",
    "    def test(self, test_data=None):\n",
    "        # test\n",
    "        predictions = []\n",
    "        if test_data == None:\n",
    "            for i in range(self.X_test.shape[0]):\n",
    "                predictions.append(self.predict(self.X_test.iloc[i]))\n",
    "            print(f'Accuracy = {(self.y_test == np.array(predictions)).sum()/len(predictions)}')\n",
    "    \n",
    "    def get_params(self):\n",
    "        #return parameters\n",
    "        return self.class_probabs, self.vocab, self.X_train, self.y_train"
   ]
  },
  {
   "source": [
    "## Preprocessing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5572, 2)\n  labels                                                sms\n0    ham  Go until jurong point, crazy.. Available only ...\n1    ham                      Ok lar... Joking wif u oni...\n2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3    ham  U dun say so early hor... U c already then say...\n4    ham  Nah I don't think he goes to usf, he lives aro...\n\nTraining data: 4457\nTest data: 1115\n"
     ]
    }
   ],
   "source": [
    "mnb = MyMultinomialNB('smsspamcollection/SMSSpamCollection')\n",
    "X_train, y_train, X_test, y_test = mnb.preprocess()"
   ]
  },
  {
   "source": [
    "## Fit model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class Probabilities:\nham     0.869419\nspam    0.130581\nName: labels, dtype: float64\nVocabulary length: 7823\n"
     ]
    }
   ],
   "source": [
    "mnb.fit()"
   ]
  },
  {
   "source": [
    "## Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.9632286995515695\n"
     ]
    }
   ],
   "source": [
    "mnb.test()"
   ]
  }
 ]
}